#############################################################
#                                                           #
# This file is an essential part of collector's execution!  #
# And is responsible to get the functions:                  #
#   * subdomain_recon                                       #
#   * adjust_files                                          #
#                                                           #
#############################################################            

subdomains_recon(){
    if [ -d "${tmp_dir}" ]; then
        
        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing alienvault... "
        curl -A "${curl_agent}" -s "https://otx.alienvault.com/api/v1/indicators/domain/${domain}/passive_dns" \
            | jq --raw-output '.passive_dns[]?.hostname' 2> /dev/null | sort -u >> "${tmp_dir}/alienvault_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing amass... "
        ${amass_bin} enum -src -timeout "${amass_timeout_execution}" -d "${domain}" > "${tmp_dir}/amass_output.txt" 2> "${tmp_dir}/error.log"
        ${amass_bin} enum -passive -src -timeout "${amass_timeout_execution}" -d "${domain}" > "${tmp_dir}/amass_passive_output.txt" 2> "${tmp_dir}/error.log"
        echo "Done!"

        if [[ -n ${binaryedge_api_url} ]] && [[ -n "${binaryedge_api_key}" ]]; then
            binaryedge_api_check=$(curl -A "${curl_agent}" -s -w %{http_code} "${binaryedge_api_url}/${domain}" -H "X-key: ${binaryedge_api_key}" -o /dev/null)
            if [ "${binaryedge_api_check}" -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing binaryedge.io... "
                curl -A "${curl_agent}" -s "${binaryedge_api_url}/${domain}" \
                    -H 'X-Key:'${binaryedge_api_key}'' | jq --raw-output -r '.events[]?' 2> /dev/null \
                    | sort -u >> "${tmp_dir}/binaryedge_output.txt"
            echo "Done!"
            fi
        fi

        if [[ -n "${censys_api_url}" ]] && [[ -n "${censys_api_id}" ]] && [[ -n "${censys_api_secret}" ]]; then
            censys_api_check=$(curl -A "${curl_agent}" -s -w %{http_code} -u "${censys_api_id}:${censys_api_secret}" -H 'Content-Type: application/json' -H 'Accept: application/json' -L -X POST "${censys_api_url}" --data-raw '{"query": "parsed.names: '${domain}'", "fields": ["parsed.names"]}' -o /dev/null 2> /dev/null)
            if [ "${censys_api_check}" -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing censys.io... "
                curl -A "${curl_agent}" -s -u "${censys_api_id}:${censys_api_secret}" \
                    -H 'Content-Type: application/json' \
                    -H 'Accept: application/json' \
                    -L -X POST "${censys_api_url}" --data-raw '{"query": "parsed.names: '${domain}'", "fields": ["parsed.names"]}' \
                    | jq '.results | .[] | .[]' 2> /dev/null | grep "${domain}" | sed -e '/,/d' -e 's/^[[:blank:]]*//' -e 's/"//g' \
                    | sort -u >> "${tmp_dir}/censys_output.txt"
                echo "Done!"
            fi
        fi

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing certspotter... "
        curl -A "${curl_agent}" -s "https://certspotter.com/api/v0/certs\?domain=${domain}&include_subdomains=true&expand=dns_names" \
            | jq --raw-output -r '.[].dns_names[]' 2> /dev/null | sed 's/\"//g' | sed 's/\*\.//g' \
            | sort -u | grep "${domain}" >> "${tmp_dir}/certspotter_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing commoncrawl... "
        commoncrawl_url="http://index.commoncrawl.org/collinfo.json"
        commoncrawl_status=$(curl -A "${curl_agent}" -w "%{http_code}\n" -s -o /dev/null "${commoncrawl_url}")
        commoncrawl_raw_file="${tmp_dir}/commoncrawl_raw_output.txt"
        commoncrawl_domains_file="${tmp_dir}/commoncrawl_domains_output.txt"

        if [[ ${commoncrawl_status} == 200 ]]; then
            commoncrawl_db=$(curl -A "${curl_agent}" -s "${commoncrawl_url}" | jq --raw-output .[0]'."cdx-api"' 2> /dev/null)
            curl -A "${curl_agent}" -s "${commoncrawl_db}?url=*.${domain}/&output=json" | jq --raw-output .url? 2> /dev/null | sed 's/\*\.//g' > "${commoncrawl_raw_file}"
            cat "${commoncrawl_raw_file}" | sed -e 's_https*://__' -e "s/\/.*//" -e 's/:.*//' -e 's/^www\.//' -e "/@/d" -e 's/\.$//' \
                | sort -u > "${commoncrawl_domains_file}"
        fi
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing crt.sh... "
        curl -H "${curl_agent}" -s https://crt.sh/?q=%25."${domain}"\&output=json | jq -r '.[].name_value' 2> /dev/null | \
            sed 's/\*\.//g' | sort -u >> "${tmp_dir}/crtsh_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing dns bufferover... "
        curl -H "${curl_agent}" -k -s https://dns.bufferover.run/dns?q="${domain}" \
            | jq -r '.FDNS_A[]' 2> /dev/null \
            | sed -e "s/\"//" -e "s/\",$//" -e "s/\"//" -e "s/\t\t//" \
            | grep -E "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" | awk -F"," '{print $2}' \
            | sort -u >> "${tmp_dir}/dnsbufferover_output.txt"
        echo "Done!"
        
        if [[ -n "${dnsdb_api_url}" ]] && [[ -n "${dnsdb_api_key}" ]]; then
            dnsdb_api_check=$(curl -A "${curl_agent}" -gs -w "%{http_code}\n" -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" "${dnsdb_api_url}/*.${domain}?limit=1000000000" -o /dev/null)
            if [ "${dnsdb_api_check}"  -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing dnsdb... "
                curl -A "${curl_agent}" -gs -H "Accept: application/json" -H "X-API-Key: ${dnsdb_api_key}" "${dnsdb_api_url}/*.${domain}?limit=1000000000" \
                | jq --raw-output -r .rrname? 2> /dev/null | sed -e 's/\.$//' | sort -u > "${tmp_dir}/dnsdb_output.txt"
                echo "Done!"
            fi
        fi

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing dns dumpster... "
        dnsdumpster_url="https://dnsdumpster.com"
        dnsdumpster_csrf=$(curl -A "${curl_agent}" -s "${dnsdumpster_url}" | grep -P  "csrfmiddlewaretoken" | grep -Po '(?<=value=")[^"]*(?=")')

        curl -A "${curl_agent}" -s --cookie "csrftoken=${dnsdumpster_csrf}" -H "Referer: ${dnsdumpster_url}" \
            --data "csrfmiddlewaretoken=${dnsdumpster_csrf}&targetip=${domain}" ${dnsdumpster_url} \
            | grep -Po '<td class="col-md-4">\K[^<]*' \
            | sort -u | grep "${domain}" > "${tmp_dir}/dnsdumpster_output.txt"

        unset dnsdumpster_url
        unset dnsdumpster_csrf
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing hackertarget... "
        curl -A "${curl_agent}" -s --request GET "${hackertarget_url}${domain}" \
            | sed 's/,.*//' \
            | sort -u > "${tmp_dir}/hackertarget_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing rapiddns... "
        curl -A "${curl_agent}" -s "https://rapiddns.io/subdomain/${domain}" \
            | grep -Po 'target="_blank">\K[^<]*' | sort -u > "${tmp_dir}/rapiddns_output.txt"
        echo "Done!"

        if [[ -n "${riskiq_api_key}" ]] && [[ -n "${riskiq_api_secret}" ]]; then
            riskiq_api_check=$(curl -A "${curl_agent}" -s -w "%{http_code}\n" -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" -o /dev/null)
            if [ "${riskiq_api_check}" -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing riskiq... "
                curl -A "${curl_agent}" -s -u "${riskiq_api_key}:${riskiq_api_secret}" "${riskiq_api_url}?query=${domain}" \
                    | jq --raw-output -r .subdomains[]? 2> /dev/null | sort -u > "${tmp_dir}/riskiq_output.txt"
                sed -i "s/$/.${domain}/" "${tmp_dir}/riskiq_output.txt"
                echo "Done!"
            fi
        fi

        if [[ -n "${securitytrails_api_key}" ]]; then
            securitytrails_api_check=$(curl -A "${curl_agent}" -s -w "%{http_code}\n" "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" -o /dev/null)
            if [ "${securitytrails_api_check}" -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing securitytrails... "
                curl -A "${curl_agent}" -s "${securitytrails_api_url}/${domain}/subdomains?apikey=${securitytrails_api_key}" \
                    | jq --raw-output -r '.subdomains[]' 2> /dev/null | sort -u > "${tmp_dir}/securitytrails_output.txt"
                sed -i "s/$/.${domain}/" "${tmp_dir}/securitytrails_output.txt"
                echo "Done!"
            fi
        fi

        if [ "${shodan_use}" == "yes" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing shodan... "
            "${shodan_bin}" search --no-color --fields hostnames hostname:"${domain}" 2> /dev/null | \
                sed -e 's/;/\n/g' -e '/^$/d' | sort -u > "${tmp_dir}/shodan_subdomain_output.txt"
            echo "Done!"
        fi

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing subfinder... "
        ${subfinder_bin} -silent -d "${domain}" > "${tmp_dir}/subfinder_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing threatcrowd... "
        curl -A "${curl_agent}" -s "https://threatcrowd.org/searchApi/v2/domain/report/?domain=${domain}" \
            | jq --raw-output -r '.subdomains[]?' 2> /dev/null | grep -Eo "\b[A-Za-z0-9.-]+\.[A-Za-z]{2,6}\b" \
            | grep -P "${domain}" | sort -u > "${tmp_dir}/threatcrowd_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing threatminer... "
        curl -A "${curl_agent}" -s "https://api.threatminer.org/v2/domain.php?q=${domain}&rt=5" \
            | jq --raw-output -r '.results[]?' 2> /dev/null | sort -u > "${tmp_dir}/threatminer_output.txt"
        echo "Done!"

        if [[ -n "${virustotal_api_url}" ]] && [[ -n "${virustotal_api_key}" ]]; then
            virustotal_api_check=$(curl -A "${curl_agent}" -s -w "%{http_code}\n" "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" -o /dev/null)
            if [ "${virustotal_api_check}" -eq 200 ]; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing virus total... "
                curl -A "${curl_agent}" -s "${virustotal_api_url}${virustotal_api_key}/&domain=${domain}" \
                    | jq --raw-output -r '.subdomains[]?' 2> /dev/null | sort -u > "${tmp_dir}/virustotal_output.txt"
                echo "Done!"
            fi
        fi

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing webarchive... "
        curl -A "${curl_agent}" -s "http://web.archive.org/cdx/search/cdx?url=*.${domain}/*&output=text&fl=original&collapse=urlkey" \
            | sed -e 's_https*://__' -e "s/\/.*//" -e 's/:.*//' -e 's/^www\.//' | sed "/@/d" | sed -e 's/\.$//' \
            | sort -u > "${tmp_dir}/webarchive_output.txt"
        echo "Done!"
        
        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} We will execute brute force dns with amass, gobuster and dnssearch ${#dns_wordlists[@]} time(s)."
            echo -e "\t Take a break as this step takes a while."

            for list in "${dns_wordlists[@]}"; do
                index=$(printf "%s\n" "${dns_wordlists[@]}" | grep -En "^${list}$" | awk -F":" '{print $1}')
                if [ -s "${list}" ]; then
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}... "
                    ${amass_bin} enum -src -w "${list}" -d "${domain}" > "${tmp_dir}"/amass_brute_output_"${index}".txt 2> "${tmp_dir}/error.log"
                    ${gobuster_bin} dns -z -q -r "${resolver_dns}" -t "${gobuster_threads}" \
                        -d "${domain}" -w "${list}" > "${tmp_dir}"/gobuster_dns_output_"${index}".txt 2> "${tmp_dir}/error.log"
                    ${dnssearch_bin} -consumers 600 -domain "${domain}" -wordlist "${list}" | \
                        grep "${domain}" > "${tmp_dir}"/dnssearch_output_"${index}".txt 2> "${tmp_dir}/error.log"
                    echo "Done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}, error: ${list} does not exist or is empty!"
                    continue
                fi
                unset index
            done
            unset list
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Brute force execution of amass, gobuster and dnssearch is done."
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script"
        exit 1
    fi
    
    for ns in $(dig ns "${domain}" 2> /dev/null | grep "^${domain}\." | awk '{print $5}' | sed -e 's/\.$//'); do
        if ! dig axfr "@${ns}" "${domain}" 2> /dev/null | \
            grep -Ei "Transfer failed.|servers could be reached|timed out." > /dev/null 2>&1; then
            dig axfr "@${ns}" "${domain}" >> "${tmp_dir}/zone_transfer.txt"
        fi
    done
}

adjust_files(){
    if [ -d "${tmp_dir}" ] && [ -d "${report_dir}" ]; then
        echo -en "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Putting all domain search results in one file... "

        if [ -s "${tmp_dir}/alienvault_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/alienvault_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/amass_output.txt" ]; then
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_output.txt" | \
                grep "${domain}" | awk '{print $2}' | grep -E "^*.\.${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            grep -A 1000 "OWASP Amass.*OWASP/Amass" "${tmp_dir}/amass_output.txt" >> "${report_dir}/amass_blocks_output.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -vE "^([0-9a-zA-Z]{1,4}:)" | sed '/0.0.0.0\/0/d' >> "${tmp_dir}/tmp_blocks.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -E "^([0-9a-zA-Z]{1,4}:)" >> "${report_dir}/ipv6_blocks.txt"
        fi

        if [ -s "${tmp_dir}/amass_passive_output.txt" ]; then
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_passive_output.txt" | \
                grep "${domain}" | awk '{print $2}' | grep -E "^*.\.${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/amass_intel.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/amass_intel.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi
        
        if [ -s "${tmp_dir}/binaryedge_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/binaryedge_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/censys_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/censys_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/certspotter_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/certspotter_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/commoncrawl_domains_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/commoncrawl_domains_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/crtsh_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/crtsh_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi
        
        if [ -s "${tmp_dir}/dnsbufferover_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/dnsbufferover_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/dnsdb_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/dnsdb_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/dnsdumpster_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/dnsdumpster_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/hackertarget_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/hackertarget_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/rapiddns_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/rapiddns_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/riskiq_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/riskiq_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/securitytrails_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/securitytrails_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/shodan_subdomains_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/shodan_subdomains_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/subfinder_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/subfinder_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/threatcrowd_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/threatcrowd_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/threatminer_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/threatminer_output.txt" >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ -s "${tmp_dir}/webarchive_output.txt" ]; then
            grep -E "^*.\.${domain}" "${tmp_dir}/webarchive_output.txt" 2> /dev/null >> "${tmp_dir}/domains_tmp.txt"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            files_amass=($(ls -1A "${tmp_dir}/" | grep "amass_brute_output" 2> /dev/null))
            for f in "${files_amass[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    grep -Ev "Starting.*names|Querying.*|Average.*performed" "${file}" \
                        | grep "${domain}" | awk '{print $2}' | grep -E "^*.\.${domain}" \
                        | sort -u >> "${tmp_dir}/domains_tmp.txt"
                fi
                unset file
            done

            files_gobuster_dns=($(ls -1A "${tmp_dir}/" | grep "gobuster_dns_output" 2> /dev/null))
            for f in "${files_gobuster_dns[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' \
                        | grep -E "^*.\.${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
                fi
                unset file
            done

            files_dnssearch=($(ls -1A "${tmp_dir}/" | grep "dnssearch_output_" 2> /dev/null))
            for f in "${files_dnssearch[@]}"; do
                file="${tmp_dir}"/"${f}"
                if [[ -s "${file}" ]]; then
                    awk '{print $1}' "${file}" | tr '[:upper:]' '[:lower:]' \
                        | grep -E "^*.\.${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
                fi
                unset file
            done
        fi
        echo "Done!"

        if [ -s "${tmp_dir}/domains_tmp.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains... "
            # Removing duplicated subdomains
            sed -i 's/^\.// ; s/^-// ; s/^\:// ; s/\.\./\./ ; s/^http:\/\/// ; s/^https:\/\/// ; s/ // ; s/^$// ; /^[[:space:]]*$/d' \
                "${tmp_dir}/domains_tmp.txt"

            if tr '[:upper:]' '[:lower:]' < "${tmp_dir}/domains_tmp.txt" | sort -u > "${report_dir}/domains_found.txt" ; then

                sed -i '/owasp.*nonce/d ; /_domainkey/d' "${report_dir}/domains_found.txt"

                # Domains and subdomains resolution
                while IFS= read -r subdomain; do
                    timeout 5s host -W 3 -t A "${subdomain}" >> "${tmp_dir}/domains_host_resolution.txt"
                done < "${report_dir}/domains_found.txt"
                unset subdomain

                "${massdns_bin}" -q -r "${resolvers_wordlist}" -t A -o S \
                    -w "${tmp_dir}/domains_massdns_resolution.txt" "${report_dir}/domains_found.txt" > /dev/null 2>&1

                sed -i 's/\. A/ A/g ; s/\. CNAME/ CNAME/g' "${tmp_dir}/domains_massdns_resolution.txt"

                # Removing private IPs
                grep -E '(192\.168|10\.|172\.1[6789]\.|172\.2[0-9]\.|172\.3[01]\.)' "${tmp_dir}/domains_host_resolution.txt" \
                    | awk '{print $1"\t"$4}' >> "${tmp_dir}/domains_tmp_internal_ips.txt"
                grep -E '(192\.168|10\.|172\.1[6789]\.|172\.2[0-9]\.|172\.3[01]\.)' "${tmp_dir}/domains_massdns_resolution.txt" \
                    | awk '{print $1"\t"$3}' >> "${tmp_dir}/domains_tmp_internal_ips.txt"

                sed -i '/192\.168/d ; /10\./d ; /172\.1[6789]\./d ; /172\.2[0-9]\./d ; /172\.3[01]\./d' \
                    "${tmp_dir}/domains_host_resolution.txt"
                sed -i '/192\.168/d ; /10\./d ; /172\.1[6789]\./d ; /172\.2[0-9]\./d ; /172\.3[01]\./d' \
                    "${tmp_dir}/domains_massdns_resolution.txt"

                sort -u -o "${report_dir}/domains_internal_ipv4.txt" "${tmp_dir}/domains_tmp_internal_ips.txt"

                # Sorting out...
                # Domains with IPs
                grep -E "${IPv4_regex}$" "${tmp_dir}/domains_host_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$4}' >> "${report_dir}/domains_external_ipv4.txt"
                grep -E "${IPv6_regex}$" "${tmp_dir}/domains_host_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$4}' >> "${report_dir}/domains_external_ipv6.txt"

                grep -E "${IPv4_regex}$" "${tmp_dir}/domains_massdns_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$3}' >> "${report_dir}/domains_external_ipv4.txt"
                grep -E "${IPv6_regex}$" "${tmp_dir}/domains_massdns_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$3}' >> "${report_dir}/domains_external_ipv6.txt"

                for d in $(cat "${report_dir}/domains_found.txt"); do 
                    ipv6=($(dig "${d}" AAAA +short +time=5 +tries=3 @"${resolver_dns}" | grep -E "${IPv6_regex}"))
                    if [ -n "${ipv6}" ]; then
                        echo -e "${d}\t${ipv6}" >> "${report_dir}/domains_external_ipv6.txt"
                    fi
                    unset d
                    unset ipv6
                done

                # Domains aliases
                grep "alias" "${tmp_dir}/domains_host_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$6}' >> "${report_dir}/domains_aliases.txt"
                grep "CNAME" "${tmp_dir}/domains_massdns_resolution.txt" \
                    | sort -u | awk '{print $1"\t"$3}' >> "${report_dir}/domains_aliases.txt"
                    
                # Unavailable domains
                grep -E "NXDOMAIN|SERVFAIL|has.*record" "${tmp_dir}/domains_host_resolution.txt" \
                    | sed -e 's/^Host //' \
                    | sort -u \
                    | awk '{print $1}' >> "${report_dir}/domains_without_resolution.txt"

                # Get the domains that have a name resolution
                if [ -s "${report_dir}/domains_external_ipv4.txt" ]; then
                    awk '{print $1}' "${report_dir}/domains_external_ipv4.txt" | grep "${domain}" | sed -e '/_/d' >> "${report_dir}/domains_alive.txt"
                fi
                if [ -s "${report_dir}/domains_external_ipv6.txt" ]; then
                    awk '{print $1}' "${report_dir}/domains_external_ipv6.txt" | grep "${domain}" | sed -e '/_/d' >> "${report_dir}/domains_alive.txt"
                fi

                sort -u -o "${report_dir}/domains_alive.txt" "${report_dir}/domains_alive.txt"
                
                for d in $(cat "${report_dir}/domains_alive.txt"); do
                    sed -i "/${d}/d" "${report_dir}/domains_without_resolution.txt"
                done

            else
                echo " "
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains!"
                exit 1
            fi     
            echo "Done!"
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The file with all domains from initial recon does not exist or is empty."
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Look all files from initial recon in ${tmp_dir} and fix the problem!"
            exit 1
        fi

        if [ ${#excluded[@]} -gt 0 ] && [ -s "${report_dir}/domains_alive.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Excluding the subdomains from command line option... "
            for subdomain in "${excluded[@]}"; do
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_aliases.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_found.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_alive.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_internal_ipv4.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_external_ipv4.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_external_ipv6.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_without_resolution.txt"
            done
            unset subdomain
            echo "Done!"
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

hdc(){
    # Horizontal Domain Correlation
    # Find all possible domains
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for Horizontal Domain Correlation... "
    emails=($(whois "${domain}" | grep -E "Registrant Email|^e-mail" | egrep -ho "[[:graph:]]+@[[:graph:]]+"))

    for email in "${emails[@]}"; do
        # Domain Eye is a paid service, if you have an access
        # and would like to collaborate please PR
        # https://domaineye.com/reverse-whois/

        domains_reversewhois+=($(curl -sk -A "${curl_agent}" "https://www.reversewhois.io/?searchterm=${email}" | \
            "${html2text_bin}" | grep -E "^[0-9]"| awk '{print $2}' | sed 's/|//'))
        domains_found+=("${domains_reversewhois[@]}")

        domains_viewdns+=($(curl -sk -A "${curl_agent}" "https://viewdns.info/reversewhois/?q=${email}" | "${html2text_bin}" | \
            grep -Po "[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)" | \
            grep -Evi "viewdns|${email}"))
        domains_found+=("${domains_viewdns[@]}")
    done

    if [ ${#domains_found[@]} -gt 0 ]; then
        domains_horizontal+=($(echo ${domains_found[@]} | tr ' ' '\n' | sort -u))
        printf "%s\n" "${domains_horizontal[@]}" >> "${report_dir}/domains_correlation_found.txt"
    fi

    for d in ${domains_horizontal[@]}; do 
        ipv4=($(dig "${d}" A +short +time=5 +tries=3 @"${resolver_dns}"))
        ipv6=($(dig "${d}" AAAA +short +time=5 +tries=3 @"${resolver_dns}" | grep -E "${IPv6_regex}"))
        if [ -n "${ipv4}" ]; then
            echo -e "${d}\t${ipv4}" >> "${report_dir}/domains_correlation_ipv4.txt"
        elif [ -n "${ipv6}" ]; then
            echo -e "${d}\t${ipv6}" >> "${report_dir}/domains_correlation_ipv6.txt"
        fi
    done
    echo "Done!"
}
